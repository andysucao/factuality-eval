{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86aeda5-a528-417a-99d5-77e088acb06f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import ray\n",
    "import pandas as pd\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import ChatAnyscale, ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "import os\n",
    "from multichoice import Multichoice\n",
    "from prompt_mgr import PromptMgr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79a50d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_dotenv('.env')\n",
    "oai_key = os.environ['OPENAI_API_KEY']\n",
    "ae_key = os.environ['AE_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533ecc85-ba9d-4718-a70c-2575e3ad6a78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_json('resources/datasets/val_sentence_pairs.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db7012c-acc0-48b1-bbb0-44d14b4a7c72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_short_names = {'openai://gpt-3.5-turbo' : 'gpt35', \n",
    "                    'openai://gpt-4': 'gpt4', \n",
    "                    'meta-llama/Llama-2-7b-chat-hf': 'llama7', \n",
    "                    'meta-llama/Llama-2-13b-chat-hf': 'llama13',\n",
    "                    'meta-llama/Llama-2-70b-chat-hf': 'llama70'\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d51351-5347-46a7-8d6d-b8895d6a73f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_to_test = ['openai://gpt-3.5-turbo', 'openai://gpt-4', 'meta-llama/Llama-2-7b-chat-hf', \n",
    "                  'meta-llama/Llama-2-13b-chat-hf', 'meta-llama/Llama-2-70b-chat-hf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21d50ee-46e1-4885-b2ba-d4d1d3a374de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can also try answer_last, or reduce_bias here as well. Results do not substantially change. \n",
    "pm = PromptMgr(src_dir = 'resources/environments/answer_first/prompts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1688a1a1-2400-4f40-b3e2-d93fef8296e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_model(row, model_name, prompt_mgr, swap_answers = False):\n",
    "    if swap_answers: \n",
    "        prompt = prompt_mgr.bind('consistent').render(\n",
    "                    article_sent=row['article_sent'], \n",
    "                    option_a=row['incorrect_sent'],\n",
    "                    option_b=row['correct_sent'])\n",
    "    else: \n",
    "        prompt = prompt_mgr.bind('consistent').render(\n",
    "                    article_sent=row['article_sent'], \n",
    "                    option_a=row['correct_sent'],\n",
    "                    option_b=row['incorrect_sent'])\n",
    "\n",
    "    system_prompt = prompt_mgr.bind('system').render()\n",
    "    if model_name.startswith('openai://'): \n",
    "        model_name = model_name.replace('openai://','')\n",
    "        # Needs lots of retries due to rate limiting. \n",
    "        model = ChatOpenAI(model_name=model_name, openai_api_key = oai_key, temperature = 0, max_retries = 35)\n",
    "    else: \n",
    "        model = ChatAnyscale(model_name=model_name, anyscale_api_key = ae_key, temperature = 0, max_retries = 35)\n",
    "                             \n",
    "    messages = [SystemMessage(content=system_prompt),\n",
    "                HumanMessage(content=prompt)]\n",
    "    output = model(messages)\n",
    "    return {'output': output.content } \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d32b7c-783f-44f0-891e-90a8d90bac24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's test this. \n",
    "result = query_model(df.loc[0], models_to_test[0], pm, swap_answers = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e61b26-4e70-4cc6-90ae-afa9dbc1f60f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecf7e4d-34fe-484a-9bae-5aefdfa68c03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ModelQuery:\n",
    "    def __init__(self, model, pm, swap_answers = False):\n",
    "        self.model = model\n",
    "        self.pm = pm\n",
    "        self.swap_answers = swap_answers\n",
    "        \n",
    "    def __call__(self, row):\n",
    "        return query_model(row, self.model, self.pm, self.swap_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0943a99a-5a4b-44bf-80e9-2cea5300ea24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def convert_to_pandas(ds):\n",
    "    return ds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695c89ab-1c76-44e8-a857-f062a04567b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_shards = 3 # Reasonable number. We could split more finely if we wanted to. \n",
    "num_cpus = 0.1 # A guess at the overhead of making these calls concurrently. \n",
    "ds_by_model = [None] * len(models_to_test)*2\n",
    "ds = ray.data.from_pandas(df).repartition(num_shards)\n",
    "for i in range(len(models_to_test)): \n",
    "    # We set up two tasks at the same time one for the normal and one for the swap. \n",
    "    # Each instance has to have its own copy of the prompt manager to ensure they don't overwrite one another. \n",
    "    mq = ModelQuery(models_to_test[i], pm)\n",
    "    ds_by_model[i]= ds.map(mq, num_cpus=num_cpus)\n",
    "    \n",
    "    mq_swap = ModelQuery(models_to_test[i], pm, swap_answers=True)\n",
    "    ds_by_model[i+len(models_to_test)] = ds.map(mq_swap, num_cpus=num_cpus)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfcccd4-b13b-463b-b9ba-30786e5141bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "st = time.time() \n",
    "futures = [convert_to_pandas.remote(ds) for ds in ds_by_model]\n",
    "\n",
    "results = ray.get(futures) \n",
    "et = time.time()\n",
    "print(f'Gathering results took {et-st} wall clock seconds.')\n",
    "# Typical time is about 700 seconds on a g5.12xlarge \n",
    "# Expect a few internal server errors, bad gateways, rate limits etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc77f0d-97c1-4839-9f5c-809c886f4a2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now assign the results to the table (we had to put both in the ray.get() call -- so the \n",
    "# first half is unswapped, second half is swapped. \n",
    "\n",
    "for i in range(len(models_to_test)):\n",
    "    df[model_short_names[models_to_test[i]]] = results[i]\n",
    "for i in range(len(models_to_test), 2*len(models_to_test)):\n",
    "    df[model_short_names[models_to_test[i-len(models_to_test)]]+'-swap'] = results[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c006099a-fd7d-4c68-86a5-2f619274084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a good point to save your queries and after this just focus on the data processing without having to requery. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da3a24f-14c8-467a-a97f-5fdcbf980e76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_json('llm_fact.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90309acd-f327-40f1-b07d-01353ee4336b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('llm_fact.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8057375f-f073-47be-8ccd-ff8f88a045a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Cleaner:\n",
    "    \"\"\" This cleaning class helps with \n",
    "    cleanin a given column with the given prompt manager and \n",
    "    Anyscale Endpoints key.\n",
    "    \"\"\"\n",
    "    def __init__(self, col, pm, ae_key):\n",
    "        self.col = col\n",
    "        self.pm = pm\n",
    "        self.ae_key = ae_key\n",
    "        \n",
    "    def __call__(self, row):\n",
    "        mc = Multichoice(self.pm, self.ae_key)\n",
    "        output =  mc.extract_choice(row[self.col])\n",
    "        return {'output': output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf89400-cafb-4332-8c11-c25f28a43520",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_relabel = ['gpt4', 'gpt4-swap', 'gpt35', 'gpt35-swap', 'llama7', 'llama7-swap', 'llama13', 'llama13-swap', 'llama70', 'llama70-swap']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbb99bc-b539-4f77-b746-9dd6c05ac9b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_shards = 10 # Reasonable number. We could split more finely if we wanted to. \n",
    "num_cpus = 0.1 # A guess at the overhead of making these calls concurrently. \n",
    "cleaned = [None] * len(to_relabel)\n",
    "ds = ray.data.from_pandas(df).repartition(num_shards)\n",
    "for i in range(len(to_relabel)): \n",
    "    # We set up two tasks at the same time one for the normal and one for the swap. \n",
    "    col = to_relabel[i] \n",
    "    cleaner = Cleaner(col, pm, ae_key)\n",
    "    cleaned[i]= ds.map(cleaner, num_cpus=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc957e42-030e-48cd-9978-6b17d7c34cc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "st = time.time() \n",
    "futures = [convert_to_pandas.remote(ds) for ds in cleaned]\n",
    "results = ray.get(futures) \n",
    "et = time.time()\n",
    "print(f'Cleaning took {et-st} wall clock seconds.')\n",
    "# Typical time is about 700 seconds on a g5.12xlarge \n",
    "# Expect a few internal server errors, bad gateways, rate limits etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9ec02a-b063-42d3-ae31-6e40872a420a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(to_relabel)):\n",
    "    df[to_relabel[i]+'-clean'] = results[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a837e96-be2c-4a61-95e5-f649ba7bba36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def is_correct(row): \n",
    "    if (row[0] == 'A' and row[1] == 'B'):\n",
    "        return 'Y'\n",
    "    if (row[0] == row[1]):\n",
    "        return row[0]*2\n",
    "    return 'N'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59849a8e-56d0-4917-b989-40270b13b1d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['gpt4-clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75cbddf-86ce-43a6-883d-30945dc03af3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for sn in ['gpt35', 'gpt4', 'llama7', 'llama13', 'llama70']:\n",
    "    df[sn+'-correct'] = df[[sn +'-clean', sn+'-swap-clean']].apply(is_correct, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dfa5d3-c95e-4303-8481-9afdc468ca42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[['gpt35', 'gpt35-swap', 'gpt35-clean', 'gpt35-swap-clean', 'gpt35-correct']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea135111-d8ea-4cac-8a82-39c8f2a408b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def acc_bias(col):\n",
    "    results = df[col].value_counts()/len(df)\n",
    "    acc = results['Y']\n",
    "    if 'AA' not in results:\n",
    "        results['AA'] = 0\n",
    "    if 'BB' not in results: \n",
    "        results['BB'] = 0\n",
    "        \n",
    "    bias = abs(results['AA'] - results['BB'])\n",
    "    aa_ratio = results['AA'] * 100\n",
    "    bb_ratio = results['BB'] * 100\n",
    "    if results['AA'] > results['BB']:\n",
    "        towards = 'A'\n",
    "    else:\n",
    "        towards = 'B'\n",
    "        \n",
    "    return acc, aa_ratio, bb_ratio, bias, towards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0bfc7a-ceaa-4a62-a7c7-5fd90aea2d23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for m in ['gpt35', 'gpt4', 'llama7', 'llama13', 'llama70']:\n",
    "    acc, aa_ratio, bb_ratio, bias, towards = acc_bias(m+'-correct')\n",
    "    acc = acc*100\n",
    "    bias = bias*100\n",
    "    print(f'{m}:   \\tAccuracy: {acc:.1f}%  \\tAA: {aa_ratio:.1f}%\\tBB: {bb_ratio:.1f}%\\tBias: {bias:.1f}% towards {towards}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
